{"cells":[{"cell_type":"markdown","metadata":{"id":"3I-jYczKysl6"},"source":["# pip install"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39900,"status":"ok","timestamp":1691946402596,"user":{"displayName":"Wuttinan Longjaroen","userId":"13819573670583055727"},"user_tz":-420},"id":"-dbeTdq6s2Oi","outputId":"5d49c072-3d15-4d22-a220-2238fd839a1d"},"outputs":[],"source":["!pip install -q datasets transformers sentencepiece evaluate jiwer rouge-score sacrebleu\n","!pip install --upgrade accelerate"]},{"cell_type":"markdown","metadata":{"id":"Isa3gV_xPBIj"},"source":["# General setting"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7244,"status":"ok","timestamp":1691946409830,"user":{"displayName":"Wuttinan Longjaroen","userId":"13819573670583055727"},"user_tz":-420},"id":"uswbLg-VAdYT","outputId":"f06c93f7-d1a6-4097-b02c-a77607ebb12e"},"outputs":[],"source":["import os\n","import torch\n","import numpy as np\n","import random\n","from transformers import AutoTokenizer, set_seed\n","\n","os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","\n","# training device\n","device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","\n","# pretrain model\n","size = 'base' # 'small', 'base'\n","model_checkpoint = f'google/mt5-{size}'\n","\n","# training parameters\n","num_epochs = 20 # 10, 20\n","batch_size = 8\n","learning_rate = 2e-5 # 1e-3, 2e-5\n","linear_layer_lr = 1e-3 # 2e-5\n","optimizer_name = \"adamw_torch\" # \"adamw_torch\", \"adafactor\"\n","\n","# seed\n","seed = 112\n","torch.manual_seed(seed)\n","random.seed(seed)\n","np.random.seed(seed)\n","g = torch.Generator()\n","g.manual_seed(seed)\n","set_seed(seed)\n","\n","# PairConcat parameters\n","method = 'PairConcat'\n","max_length = 32\n","model_name = f'{method}-{size}'\n","\n","# report files\n","save_dir = f'models'\n","if not os.path.exists(save_dir): os.makedirs(save_dir)\n","train_report_file = f'{save_dir}/{model_name}-seqlen{max_length}-{optimizer_name}-lr{learning_rate}-linlr{linear_layer_lr}-{num_epochs}ep-seed{seed}-train.csv'\n","test_report_file = f'{save_dir}/{model_name}-seqlen{max_length}-{optimizer_name}-lr{learning_rate}-linlr{linear_layer_lr}-{num_epochs}ep-seed{seed}-test.csv'\n","gen_output_file = f'{save_dir}/{model_name}-seqlen{max_length}-{optimizer_name}-lr{learning_rate}-linlr{linear_layer_lr}-{num_epochs}ep-seed{seed}-gen.txt'\n","\n","# Print parameters setting\n","print(f'Training Device                    : {device}')\n","print('====================')\n","print('Pre-train')\n","print(f'Model size                         : {size}')\n","print(f'Checkpoint                         : {model_checkpoint}')\n","print('====================')\n","print('Training parameters')\n","print(f'Batch size                         : {batch_size}')\n","print(f'Epochs                             : {num_epochs}')\n","print(f'Learning rate                      : {learning_rate}')\n","print(f'Linear layer lr                    : {linear_layer_lr}')\n","print(f'Optimizer name                     : {optimizer_name}')\n","print('====================')\n","print(f'{method} parameters')\n","print(f'Model name                         : {model_name}')\n","print(f'I/O length                         : {max_length}/{max_length}')\n","print('====================')\n","print(f'Train report                       : {train_report_file}')\n","print(f'Test report                        : {test_report_file}')\n","print(f'Generated text                     : {gen_output_file}')\n","print('====================')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":697,"status":"ok","timestamp":1691221282682,"user":{"displayName":"Wuttinan Longjaroen","userId":"13819573670583055727"},"user_tz":-420},"id":"pul54NrgyT7t","outputId":"7bf27bab-595d-4545-a93c-5a362b02a1d3"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=False)"]},{"cell_type":"markdown","metadata":{"id":"XvXFSGPByqdb"},"source":["# Utility"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jXmjamr9yqBa"},"outputs":[],"source":["import heapq\n","\n","def scoring(s1, s2, gap_penalty, match, mismatch):\n","  matrix = np.zeros( (len(s1)+1, len(s2)+1) )\n","  for i in range( len(s1)+1 ):\n","    matrix[i][0] = i*gap_penalty\n","\n","  for j in range( len(s2)+1 ):\n","    matrix[0][j] = j*gap_penalty\n","\n","  trace_sequence = ''\n","\n","  for i in range(len(s1)+1):\n","    for j in range(len(s2)+1):\n","      if i == 0 and j == 0:\n","        #We're in the upper right corner\n","        matrix[i][j] = 0\n","      elif i == 0:\n","        matrix[i][j] = matrix[i][j-1] + gap_penalty\n","        trace_sequence += '-'\n","      elif j == 0:\n","        matrix[i][j] = matrix[i-1][j] + gap_penalty\n","        trace_sequence += '-'\n","      else:\n","        left = matrix[i][j-1] + gap_penalty\n","        top = matrix[i-1][j] + gap_penalty\n","\n","        if s1[i-1] == s2[j-1]:\n","          trace_sequence += s1[i-1]\n","          diagonal = matrix[i-1][j-1] + match\n","        else:\n","          trace_sequence += '-'\n","          diagonal = matrix[i-1][j-1] + mismatch\n","\n","\n","        # print(diagonal, top, left)\n","        matrix[i][j] = max(diagonal, top, left)\n","  # print(trace_sequence)\n","\n","  return matrix\n","\n","def traceback(matrix, s1, s2):\n","  i = len(s1)\n","  j = len(s2)\n","\n","  aligned_sequence = ''\n","  while i > 0 and j > 0:\n","    # current = matrix[i][j]\n","    left = matrix[i-1][j]\n","    diagonal = matrix[i-1][j-1]\n","    top = matrix[i][j-1]\n","\n","    w1 = s1[i-1]\n","    w2 = s2[j-1]\n","\n","    if left > diagonal:\n","      if left > top:\n","        # left is maximum\n","        aligned_sequence = '({},{})'.format(w1, '-') + aligned_sequence\n","        i=i-1\n","      else:\n","        # top is maximum\n","        aligned_sequence = '({},{})'.format('-', w2) + aligned_sequence\n","        j=j-1\n","    elif diagonal > top:\n","      # match or mismatch\n","      # diagonal is maximum\n","      if s1[i-1] == s2[j-1]:\n","        aligned_sequence = w1 + aligned_sequence\n","      else:\n","        # mismatch\n","        aligned_sequence = '({},{})'.format(w1, w2) + aligned_sequence\n","      i=i-1\n","      j=j-1\n","    else:\n","      # top is maximum\n","      aligned_sequence = '({},{})'.format('-', w2) + aligned_sequence\n","      j=j-1\n","\n","  while i > 0:\n","    w1 = s1[i-1]\n","    aligned_sequence = '({},{})'.format(w1, '-') + aligned_sequence\n","    i=i-1\n","\n","  while j > 0:\n","    w2 = s2[j-1]\n","    aligned_sequence = '({},{})'.format('-', w2) + aligned_sequence\n","    j=j-1\n","\n","  return aligned_sequence\n","\n","def needleman_wunsch(s1, s2, gap_penalty=-1, match=1, mismatch=-1):\n","  matrix = scoring(s1, s2, gap_penalty, match, mismatch)\n","  alignment = traceback(matrix, s1, s2)\n","  return matrix, alignment\n","\n","def n_best_alignments(seq1, seq2, n, dp,\n","                      i=None,\n","                      j=None,\n","                      aligned_seq1='',\n","                      aligned_seq2=''):\n","    if i is None or j is None:\n","        i, j = len(seq1), len(seq2)\n","\n","    if i == 0 and j == 0:\n","        return [(-dp[i][j], aligned_seq1, aligned_seq2)]\n","\n","    candidates = []\n","    if i > 0 and j > 0:\n","        if seq1[i - 1] == seq2[j - 1]:\n","            score = dp[i - 1][j - 1] + 1\n","        else:\n","            score = dp[i - 1][j - 1] - 1\n","        if dp[i][j] == score:\n","            candidates.append(\n","                ( i - 1, j - 1,\n","                  seq1[i - 1] + ' ' + aligned_seq1,\n","                  seq2[j - 1] + ' ' + aligned_seq2 ) )\n","\n","    if i > 0 and dp[i][j] == dp[i - 1][j] - 1:\n","        candidates.append(\n","            ( i - 1, j,\n","              seq1[i - 1] + ' ' + aligned_seq1,\n","              '-' + ' ' + aligned_seq2 ) )\n","\n","    if j > 0 and dp[i][j] == dp[i][j - 1] - 1:\n","        candidates.append(\n","            ( i, j - 1,\n","              '-' + ' ' + aligned_seq1,\n","              seq2[j - 1] + ' ' + aligned_seq2 ) )\n","\n","    results = []\n","    for (new_i, new_j, new_seq1, new_seq2) in candidates:\n","        results.extend(\n","            n_best_alignments(seq1, seq2, n, dp, new_i, new_j, new_seq1,\n","                              new_seq2))\n","\n","    if len(results) > n:\n","        results = heapq.nsmallest(n, results)\n","    return results\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"njXF8sSGiCY4"},"outputs":[],"source":["def n_best_align_tokenized_tokens(seq1, seq2, n=3, max_length=32):\n","  seq1 = tokenizer.convert_ids_to_tokens( seq1 )\n","  seq2 = tokenizer.convert_ids_to_tokens( seq2 )\n","\n","  dp, _ = needleman_wunsch(seq1, seq2)\n","  alignments = n_best_alignments(seq1, seq2, n, dp)\n","\n","  aligned_ttids = []\n","  attn_mask = []\n","  for alignment in alignments:\n","    _, aligned_seq1, aligned_seq2 = alignment\n","\n","    ttid1 = tokenizer.convert_tokens_to_ids( aligned_seq1.strip().split(' ') )\n","    ttid2 = tokenizer.convert_tokens_to_ids( aligned_seq2.strip().split(' ') )\n","    attn = [1] * len(ttid1)\n","\n","    pad_length = max_length - len(ttid1)\n","    if pad_length > 0:\n","      pad = [tokenizer.pad_token_id] * pad_length\n","      ttid1 = ttid1 + pad\n","\n","    pad_length = max_length - len(ttid2)\n","    if pad_length > 0:\n","      pad = [tokenizer.pad_token_id] * pad_length\n","      ttid2 = ttid2 + pad\n","\n","    pad_length = max_length - len(attn)\n","    if pad_length > 0:\n","      pad = [tokenizer.pad_token_id] * pad_length\n","      attn = attn + pad\n","\n","    aligned_ttids.append((ttid1, ttid2))\n","    attn_mask.append(attn)\n","\n","  return aligned_ttids, attn_mask"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BoNlXQ_k5PRa"},"outputs":[],"source":["from datasets import load_dataset\n","\n","def preprocess_function(examples, max_sequence_length=32):\n","    model_inputs = {}\n","\n","    input_ids = []\n","    attention_mask = []\n","    # Prepare labels\n","    ref = [s.replace('|', '') for s in examples['answer_segmented']]\n","    ref_ids = tokenizer(ref, max_length=max_length, padding='max_length',\n","                        truncation=True, return_tensors='pt')\n","    model_inputs['labels'] = ref_ids['input_ids']\n","\n","    # Prepare inputs\n","    asr_texts = [s.replace('|', '') for s in examples['asr_segmented']]\n","    bp_texts = [s.replace('|', '') for s in examples['bangphim_segmented']]\n","\n","    # Prepare n-best alignment inputs\n","    asr_ids = tokenizer(asr_texts, add_special_tokens=True)\n","    bp_ids = tokenizer(bp_texts, add_special_tokens=True)\n","    for s1_ids, s2_ids in zip(asr_ids.input_ids, bp_ids.input_ids): # Each input pairs\n","      alignments, attn_mask = n_best_align_tokenized_tokens(s1_ids, s2_ids, n=1, max_length=max_length) # dim = (1, 2, max_length)\n","\n","      alignments = torch.tensor( alignments ) # (1, 2, 32)\n","      attn_mask = torch.tensor( attn_mask ) # (1, 32)\n","\n","      input_ids.append( alignments )\n","      attention_mask.append( attn_mask )\n","\n","    model_inputs['input_ids'] = torch.stack( input_ids )\n","    model_inputs['attention_mask'] = torch.stack( attention_mask )\n","\n","    return model_inputs\n","\n","def create_tokenized_dataset(input_filepath, validate_filepath=None, test_filepath=None):\n","    data_files = {}\n","    data_files[\"train\"] = input_filepath\n","    if validate_filepath is not None: data_files[\"validate\"] = validate_filepath\n","    if test_filepath is not None: data_files[\"test\"] = test_filepath\n","\n","    dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\",\")\n","\n","    tokenized_dataset = dataset.map(\n","        preprocess_function,\n","        batched=True,\n","        num_proc=1,\n","        remove_columns=['Unnamed: 0',\n","                        'answer_segmented',\n","                        'asr_segmented',\n","                        'bangphim_segmented',\n","                        'room_id',\n","                        'alignment',\n","                        'flat_sequence',\n","                        'flat_position',\n","                        'flat_source',\n","                        'label', 'input', 'target'],\n","    )\n","    return tokenized_dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":557},"executionInfo":{"elapsed":7950,"status":"error","timestamp":1691221291347,"user":{"displayName":"Wuttinan Longjaroen","userId":"13819573670583055727"},"user_tz":-420},"id":"l1gM8v74oAbg","outputId":"cf6048f0-0207-42c5-8df2-25f55c529b9d"},"outputs":[],"source":["import evaluate\n","\n","wer = evaluate.load(\"wer\")\n","rouge = evaluate.load('rouge')\n","sbleu = evaluate.load(\"sacrebleu\")\n","meteor = evaluate.load('meteor')\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","\n","    decoded_preds, decoded_labels = [], []\n","    for pred_token_ids, label_token_ids in zip(predictions, labels):\n","        pred_tokens = [\n","            token for token in tokenizer.convert_ids_to_tokens(\n","                pred_token_ids)\n","            if token not in tokenizer.all_special_tokens\n","        ]\n","        label_tokens = [\n","            token for token in tokenizer.convert_ids_to_tokens(\n","                label_token_ids)\n","            if token not in tokenizer.all_special_tokens\n","        ]\n","        decoded_preds.append(' '.join(pred_tokens))\n","        decoded_labels.append(' '.join(label_tokens))\n","\n","    # decoded_preds = tokenizer.batch_decode( predictions, skip_special_tokens=True )\n","\n","    # # Replace -100 in the labels as we can't decode them.\n","    # labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    # decoded_labels = tokenizer.batch_decode( labels, skip_special_tokens=True )\n","\n","    # decoded_preds = [' '.join( pred.strip() ) for pred in decoded_preds]\n","    # decoded_labels = [' '.join( label.strip() ) for label in decoded_labels]\n","\n","\n","    wer_score    = wer.compute(predictions=decoded_preds, references=decoded_labels)\n","    rouge_score  = rouge.compute(predictions=decoded_preds, references=decoded_labels, tokenizer=lambda x: x.split())\n","    sbleu_score  = sbleu.compute(predictions=decoded_preds, references=decoded_labels)\n","    meteor_score = meteor.compute(predictions=decoded_preds, references=decoded_labels)\n","\n","    result = {'wer': wer_score,\n","              'rouge1': rouge_score['rouge1'],\n","              'rouge2': rouge_score['rouge2'],\n","              'rougeL': rouge_score['rougeL'],\n","              'sacrebleu': sbleu_score['score'],\n","              'meteor': meteor_score['meteor'],\n","              }\n","\n","    return {k: round(v, 4) for k, v in result.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FkCUd3pzlrNk"},"outputs":[],"source":["def generate(model, tokenizer, input_ids, decoder_input_ids=None, device='cpu', max_length=20):\n","  encoded_sequence = None\n","  if decoder_input_ids == None:\n","    decoder_input_ids = (tokenizer(\"<pad>\", add_special_tokens=False, return_tensors=\"pt\").input_ids).to(device)\n","    assert decoder_input_ids[0, 0].item() == model.config.decoder_start_token_id, \"`decoder_input_ids` should correspond to `model.config.decoder_start_token_id`\"\n","\n","  for i in range(max_length):\n","    if encoded_sequence == None:\n","      outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids, return_dict=True)\n","      encoded_sequence = (outputs.encoder_last_hidden_state,) # get encoded sequence\n","      lm_logits = outputs.logits # get logits\n","\n","      next_decoder_input_ids = torch.argmax(lm_logits[:, -1:], axis=-1) # sample last token with highest prob\n","      decoder_input_ids = torch.cat([decoder_input_ids, next_decoder_input_ids], axis=-1) # concat\n","      if next_decoder_input_ids == model.config.eos_token_id:\n","        # print('EOS occur')\n","        break\n","    else:\n","      lm_logits = model(None,\n","                    encoder_outputs=encoded_sequence,\n","                    decoder_input_ids=decoder_input_ids,\n","                    return_dict=True).logits\n","\n","      next_decoder_input_ids = torch.argmax(lm_logits[:, -1:], axis=-1) # sample last token with highest prob again\n","      decoder_input_ids = torch.cat([decoder_input_ids, next_decoder_input_ids], axis=-1) # concat again\n","      if next_decoder_input_ids == model.config.eos_token_id:\n","        # print('EOS occur')\n","        break\n","  return decoder_input_ids"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ziGZzgWKZPwZ"},"outputs":[],"source":["from transformers import MT5EncoderModel, MT5ForConditionalGeneration\n","from transformers.modeling_outputs import Seq2SeqLMOutput, BaseModelOutput\n","from torch.nn import CrossEntropyLoss\n","from typing import Optional, Tuple, Union"]},{"cell_type":"markdown","metadata":{"id":"KSR4wfKCZBI6"},"source":["## PairConcatEncoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CaC7CQZVBEQH"},"outputs":[],"source":["import torch.nn as nn\n","\n","class PairConcatEncoder(MT5EncoderModel):\n","    def __init__(self, config, encoders):\n","        super().__init__(config)\n","        del(self.shared)\n","        del(self.encoder)\n","\n","        self.linear = nn.Linear(config.d_model * 2, config.d_model)\n","        self.encoder = encoders\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        encoder_hidden_states=None,\n","        encoder_attention_mask=None,\n","        inputs_embeds=None,\n","        head_mask=None,\n","        cross_attn_head_mask=None,\n","        past_key_values=None,\n","        use_cache=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","    ):\n","          input_ids = np.squeeze( input_ids, axis=1 ) # [bs, 1, 2, 32] -> [bs, 2, 32]\n","          embedding = self.encoder.embed_tokens( input_ids )\n","          # embedding[:, 0] -> aligned seq1\n","          # embedding[:, 1] -> aligned seq2\n","          concat_embedding = torch.cat( (embedding[:, 0], embedding[:, 1]), dim=-1 ) # [32, 512][32, 512]\n","          inputs_embeds = self.linear( concat_embedding ) # (bs, 32, 512)\n","          # print('inputs_embeds: ', inputs_embeds.shape) # (bs, 32, 512)\n","          encoder_outputs = self.encoder( # MT5Stack.forward\n","              attention_mask=attention_mask,\n","              inputs_embeds=inputs_embeds,\n","              head_mask=head_mask,\n","              output_attentions=output_attentions,\n","              output_hidden_states=output_hidden_states,\n","              return_dict=return_dict,\n","          )\n","          return encoder_outputs\n","\n","class PairConcat(MT5ForConditionalGeneration):\n","    def __init__(self, config):\n","        super().__init__(config)\n","        pretrain = MT5ForConditionalGeneration.from_pretrained(model_checkpoint)\n","        del(self.shared)\n","        del(self.encoder)\n","        del(self.decoder)\n","        del(self.lm_head)\n","\n","        self.encoder = PairConcatEncoder(config, pretrain.encoder)\n","        self.decoder = pretrain.decoder\n","        self.lm_head = pretrain.lm_head\n","\n","    def get_encoder(self):\n","      return self.encoder\n","\n","    def forward(\n","        self,\n","        input_ids: Optional[torch.LongTensor] = None,\n","        attention_mask: Optional[torch.FloatTensor] = None,\n","        decoder_input_ids: Optional[torch.LongTensor] = None,\n","        decoder_attention_mask: Optional[torch.BoolTensor] = None,\n","        head_mask: Optional[torch.FloatTensor] = None,\n","        decoder_head_mask: Optional[torch.FloatTensor] = None,\n","        cross_attn_head_mask: Optional[torch.Tensor] = None,\n","        encoder_outputs: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n","        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n","        inputs_embeds: Optional[torch.FloatTensor] = None,\n","        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n","        labels: Optional[torch.LongTensor] = None,\n","        use_cache: Optional[bool] = None,\n","        output_attentions: Optional[bool] = None,\n","        output_hidden_states: Optional[bool] = None,\n","        return_dict: Optional[bool] = None,\n","    ) -> Union[Tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n","\n","        use_cache = use_cache if use_cache is not None else self.config.use_cache\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        # FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask\n","        if head_mask is not None and decoder_head_mask is None:\n","            if self.config.num_layers == self.config.num_decoder_layers:\n","                # warnings.warn(__HEAD_MASK_WARNING_MSG, FutureWarning)\n","                decoder_head_mask = head_mask\n","\n","        # print(encoder_outputs)\n","        # print(return_dict)\n","        # Encode if needed (training, first prediction pass)\n","        if encoder_outputs is None:\n","            encoder_outputs = self.encoder(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                inputs_embeds=inputs_embeds,\n","                head_mask=head_mask,\n","                output_attentions=output_attentions,\n","                output_hidden_states=output_hidden_states,\n","                return_dict=return_dict,\n","            )\n","        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n","            encoder_outputs = BaseModelOutput(\n","                last_hidden_state=encoder_outputs[0],\n","                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n","                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n","            )\n","\n","        hidden_states = encoder_outputs[0]\n","\n","        if labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:\n","            # get decoder inputs from shifting lm labels to the right\n","            decoder_input_ids = self._shift_right(labels)\n","            # print('decoder_input_ids', decoder_input_ids.shape)\n","\n","        # Decode\n","        decoder_outputs = self.decoder(\n","            input_ids=decoder_input_ids,\n","            attention_mask=decoder_attention_mask,\n","            inputs_embeds=decoder_inputs_embeds,\n","            past_key_values=past_key_values,\n","            encoder_hidden_states=hidden_states,\n","            encoder_attention_mask=attention_mask,\n","            head_mask=decoder_head_mask,\n","            cross_attn_head_mask=cross_attn_head_mask,\n","            use_cache=use_cache,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        sequence_output = decoder_outputs[0]\n","\n","        lm_logits = self.lm_head(sequence_output)\n","        # print('lm_logits.shape:', lm_logits.shape)\n","        loss = None\n","        if labels is not None:\n","            loss_fct = CrossEntropyLoss(ignore_index=-100)\n","            labels = labels.to(lm_logits.device)\n","            loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1))\n","\n","        if not return_dict:\n","            output = (lm_logits,) + decoder_outputs[1:] + encoder_outputs\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return Seq2SeqLMOutput(\n","            loss=loss,\n","            logits=lm_logits,\n","            past_key_values=decoder_outputs.past_key_values,\n","            decoder_hidden_states=decoder_outputs.hidden_states,\n","            decoder_attentions=decoder_outputs.attentions,\n","            cross_attentions=decoder_outputs.cross_attentions,\n","            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n","            encoder_hidden_states=encoder_outputs.hidden_states,\n","            encoder_attentions=encoder_outputs.attentions,\n","        )\n","\n","    # Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.prepare_inputs_for_generation\n","    def prepare_inputs_for_generation(\n","        self,\n","        input_ids,\n","        past_key_values=None,\n","        attention_mask=None,\n","        head_mask=None,\n","        decoder_head_mask=None,\n","        decoder_attention_mask=None,\n","        cross_attn_head_mask=None,\n","        use_cache=None,\n","        encoder_outputs=None,\n","        **kwargs,\n","    ):\n","\n","        # cut decoder_input_ids if past is used\n","        if past_key_values is not None:\n","            input_ids = input_ids[:, -1:]\n","            # print('prepare_inputs (past not None)', tokenizer.batch_decode( input_ids, skip_special_tokens=False ))\n","            # print()\n","\n","        return {\n","            \"decoder_input_ids\": input_ids,\n","            \"past_key_values\": past_key_values,\n","            \"encoder_outputs\": encoder_outputs,\n","            \"attention_mask\": attention_mask,\n","            \"head_mask\": head_mask,\n","            \"decoder_head_mask\": decoder_head_mask,\n","            \"decoder_attention_mask\": decoder_attention_mask,\n","            \"cross_attn_head_mask\": cross_attn_head_mask,\n","            \"use_cache\": use_cache,\n","        }\n","\n","    # Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.prepare_decoder_input_ids_from_labels\n","    def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n","        return self._shift_right(labels)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dKYHExtlDBYZ"},"outputs":[],"source":["from transformers import AutoConfig\n","\n","print('Pre-train')\n","print(f'Model size                         : {size}')\n","print(f'Checkpoint                         : {model_checkpoint}')\n","print('====================')\n","print(f'{method} parameters')\n","print(f'Model name                         : {model_name}')\n","print(f'I/O length                         : {max_length}/{max_length}')\n","print('====================')\n","\n","mt5_gen_conf = AutoConfig.from_pretrained(model_checkpoint)\n","model = PairConcat(mt5_gen_conf)\n","model = model.to(device)\n","# model"]},{"cell_type":"markdown","metadata":{"id":"l5qmJGZRd8ZK"},"source":["# Dataset preparation by DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"av6sjIDkGndi"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","tokenized_dataset = create_tokenized_dataset('/data/train.csv', '/data/validate.csv', '/data/test.csv')\n","tokenized_dataset"]},{"cell_type":"markdown","metadata":{"id":"vEQ5dWBILJmv"},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rOM6Yszlao6w"},"outputs":[],"source":["train_tokenized_dataset = tokenized_dataset[\"train\"]\n","valid_tokenized_dataset = tokenized_dataset[\"validate\"]\n","\n","# prepare dataloader\n","train_tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n","train_dataloader = torch.utils.data.DataLoader(train_tokenized_dataset, batch_size=batch_size)\n","\n","valid_tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n","valid_dataloader = torch.utils.data.DataLoader(valid_tokenized_dataset, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D4seQ-cZIKzp"},"outputs":[],"source":["from transformers import Seq2SeqTrainer\n","from transformers.optimization import get_scheduler, Adafactor\n","from torch.optim import AdamW\n","\n","class CustomSeq2SeqTrainer(Seq2SeqTrainer):\n","    def __init__(self, *args, linear_layer_lr=None, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        self.linear_layer_lr = linear_layer_lr if linear_layer_lr is not None else self.args.learning_rate\n","\n","    def create_optimizer_and_scheduler(self, num_training_steps: int):\n","        # Get model parameters\n","        model = self.model\n","        no_decay = [\"bias\", \"LayerNorm.weight\"]\n","\n","        # Define four parameter groups: two custom layers and two for the remaining parameters\n","        optimizer_grouped_parameters = [\n","            {\n","                \"params\": [p for n, p in model.named_parameters() if \"linear\" in n and not any(nd in n for nd in no_decay)],\n","                \"weight_decay\": self.args.weight_decay,\n","                \"lr\": self.linear_layer_lr,\n","            },\n","            {\n","                \"params\": [p for n, p in model.named_parameters() if \"linear\" not in n and not any(nd in n for nd in no_decay)],\n","                \"weight_decay\": self.args.weight_decay,\n","                \"lr\": self.args.learning_rate,\n","            },\n","            {\n","                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.0,\n","                \"lr\": self.args.learning_rate,\n","            },\n","        ]\n","\n","        # Create the optimizer using the parameter groups\n","        if self.args.optim == 'adamw_torch':\n","            self.optimizer = AdamW(optimizer_grouped_parameters, lr=self.args.learning_rate)\n","        elif self.args.optim == 'adafactor':\n","            self.optimizer = Adafactor(optimizer_grouped_parameters, lr=self.args.learning_rate)\n","        else:\n","            raise ValueError(f\"Invalid optimizer_type: {self.args.optim}. Choose 'adam_torch' or 'adafactor'.\")\n","\n","        # Create the learning rate scheduler\n","        self.lr_scheduler = get_scheduler(\n","            self.args.lr_scheduler_type,\n","            self.optimizer,\n","            num_warmup_steps=self.args.warmup_steps,\n","            num_training_steps=num_training_steps,\n","        )\n","\n","        return self.optimizer, self.lr_scheduler\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TAvV-KNzLPHS"},"outputs":[],"source":["from transformers import Seq2SeqTrainingArguments\n","\n","args = Seq2SeqTrainingArguments(\n","    output_dir=f'./{model_name}',\n","    num_train_epochs=num_epochs,\n","\n","    learning_rate=learning_rate,\n","    lr_scheduler_type='constant',\n","\n","    logging_strategy='epoch',\n","    evaluation_strategy='epoch',\n","    # evaluation_strategy='steps',\n","    # eval_steps=100,\n","    # logging_strategy='steps',\n","    # logging_steps=100,\n","\n","    # save_strategy='no',\n","    save_strategy = 'epoch',\n","    load_best_model_at_end = True,\n","\n","    metric_for_best_model = 'wer',\n","    greater_is_better = False,\n","\n","    # metric_for_best_model = 'loss',\n","    # greater_is_better = False,\n","\n","    # metric_for_best_model = 'meteor',\n","    # greater_is_better = True,\n","\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","\n","    do_train=True,\n","    do_eval=True,\n","\n","    # optim='adafactor',\n","    # optim='adamw_torch',\n","    optim=optimizer_name,\n","\n","    predict_with_generate=True,\n","    # generation_max_length=max_length,\n","    # generation_num_beams\n","\n","    # have to use half=False to avoid loss=0\n","    # ref:https://stackoverflow.com/questions/65332165/loss-is-nan-when-fine-tuning-huggingface-nli-model-both-roberta-bart\n","    fp16=False,\n","\n","    # save the best model and the last\n","    # https://stackoverflow.com/a/67615225/3027437\n","    # https://discuss.huggingface.co/t/save-only-best-model-in-trainer/8442/4\n","    save_total_limit=2,\n",")\n","\n","# Instantiate the custom trainer\n","# trainer = Seq2SeqTrainer(\n","trainer = CustomSeq2SeqTrainer(\n","    model=model,\n","    args=args,\n","    train_dataset=tokenized_dataset['train'],\n","    eval_dataset=tokenized_dataset['validate'],\n","    compute_metrics=compute_metrics,\n","    linear_layer_lr=linear_layer_lr,\n",")\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5w-9XsX4bXuh"},"outputs":[],"source":["import pandas as pd\n","\n","eval_metrics = trainer.evaluate()\n","print(eval_metrics)\n","eval_df = pd.DataFrame(eval_metrics, index=[0])\n","eval_df.to_csv(train_report_file, index=False)"]},{"cell_type":"markdown","metadata":{"id":"j2mkBDKsOt7E"},"source":["# Test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f3nke5lWAJjT"},"outputs":[],"source":["test_tokenized_dataset = tokenized_dataset[\"test\"]\n","\n","# prepare dataloader\n","test_tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n","dataloader = torch.utils.data.DataLoader(test_tokenized_dataset, batch_size=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vB6J_2FvMGlA"},"outputs":[],"source":["test_result = trainer.predict(test_tokenized_dataset)\n","test_result.metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gsrb1MMzC-DM"},"outputs":[],"source":["results = {}\n","for k, v in test_result.metrics.items():\n","  if k in ['test_runtime','test_samples_per_second','test_steps_per_second']: continue\n","  k = k.replace('test_', '').title()\n","  results[k] = v\n","test_df = pd.DataFrame([results], index=[model_name])\n","test_df.to_csv(test_report_file, index=False)"]},{"cell_type":"markdown","metadata":{"id":"FFgwTIdYmIIK"},"source":["# Generate results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i48oGHACLD91"},"outputs":[],"source":["with open(gen_output_file, 'w', encoding='utf-8') as output_file:\n","    for i, batch in enumerate(dataloader, 1):\n","        print(f'test inputs #{i}:')\n","        print('==================')\n","\n","        output_file.write(f'test inputs #{i}:\\n')\n","        output_file.write('==================\\n')\n","\n","        input_ids = (batch['input_ids'].to(device))\n","        labels_ids = (batch['labels'].to(device))\n","\n","        inp = input_ids[:, :, :]\n","        inp1 =  np.reshape(batch['input_ids'][:, :, 0], (1, 32))\n","        inp2 =  np.reshape(batch['input_ids'][:, :, 1], (1, 32))\n","\n","        s1 = generate(model, tokenizer, input_ids, decoder_input_ids=None, device=device, max_length=max_length)\n","        s2 = model.generate(input_ids, max_length=max_length)\n","\n","        decoded_input_1 = tokenizer.batch_decode( inp1, skip_special_tokens=True )\n","        decoded_input_2 = tokenizer.batch_decode( inp2, skip_special_tokens=True )\n","\n","        decoded_preds_1 = tokenizer.batch_decode( s1, skip_special_tokens=True )\n","        decoded_preds_2 = tokenizer.batch_decode( s2, skip_special_tokens=True )\n","\n","        decoded_labels = tokenizer.batch_decode( labels_ids, skip_special_tokens=True )\n","\n","        print('input_1       :', decoded_input_1[0])\n","        print('input_2       :', decoded_input_2[0])\n","        print('labels        :', decoded_labels[0])\n","        print('generate      :', decoded_preds_1[0])\n","        print('model.generate:', decoded_preds_2[0])\n","        print()\n","\n","        output_file.write('input_1       : ' + decoded_input_1[0] + '\\n')\n","        output_file.write('input_2       : ' + decoded_input_2[0] + '\\n')\n","        output_file.write('labels        : ' + decoded_labels[0] + '\\n')\n","        output_file.write('generate      : ' + decoded_preds_1[0] + '\\n')\n","        output_file.write('model.generate: ' + decoded_preds_2[0] + '\\n')\n","        output_file.write('\\n')"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPQQgqHpbvftL1lD29dmzPL","collapsed_sections":["PvtTWblepS-k"],"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"016ab00309d34aa58a5cbea9dadfee73":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20ccd4d043a649c6b5a1b63641d68309":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a08124c65a8b4936af2933d859975001","IPY_MODEL_c664d3311fed4d5182c78494a9c63070","IPY_MODEL_ae8dd58bdd044cd0bdc41554d5481239"],"layout":"IPY_MODEL_e55af7c0c30944189700d2f9b7ff2d77"}},"2868ff1655cf4dc2896bb21e58ba3217":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4bc1869030c8457d8d33214138fda1b4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e4d33d17426d404284522f9bde72d205","IPY_MODEL_f67d717f6af64b56b8fbca28918d626e","IPY_MODEL_e7ad67bf563740b5afc37583cdea09d2"],"layout":"IPY_MODEL_016ab00309d34aa58a5cbea9dadfee73"}},"574f54ee4ecd41fe81ca7cccc1042e62":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"64f5a1b4c60f40f2ae934d87468493a6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8a21b72097c9434f8953bbb993468098":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92fd04fdbea4491594e65187635313e6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a08124c65a8b4936af2933d859975001":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ccea5de2c3b34d339059a12f442f9a55","placeholder":"​","style":"IPY_MODEL_92fd04fdbea4491594e65187635313e6","value":"Downloading (…)lve/main/config.json: 100%"}},"ae8dd58bdd044cd0bdc41554d5481239":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f6b659b42ffb456d93a09e70f5880a2e","placeholder":"​","style":"IPY_MODEL_574f54ee4ecd41fe81ca7cccc1042e62","value":" 702/702 [00:00&lt;00:00, 17.4kB/s]"}},"ba03c90f66614db2b1eb879d0dd01ffa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c664d3311fed4d5182c78494a9c63070":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f021340fe07540eb94f113e5e48b7112","max":702,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cd45648433e14fdaaff6f60167c6baf5","value":702}},"c7466203449644938b97751a8f345e5f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c84ecd1d0dc041df86f79a6b0d19277d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ccea5de2c3b34d339059a12f442f9a55":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd45648433e14fdaaff6f60167c6baf5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e4d33d17426d404284522f9bde72d205":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2868ff1655cf4dc2896bb21e58ba3217","placeholder":"​","style":"IPY_MODEL_c7466203449644938b97751a8f345e5f","value":"Downloading (…)lve/main/config.json: 100%"}},"e55af7c0c30944189700d2f9b7ff2d77":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7ad67bf563740b5afc37583cdea09d2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c84ecd1d0dc041df86f79a6b0d19277d","placeholder":"​","style":"IPY_MODEL_ba03c90f66614db2b1eb879d0dd01ffa","value":" 642/642 [00:00&lt;00:00, 8.92kB/s]"}},"f021340fe07540eb94f113e5e48b7112":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f67d717f6af64b56b8fbca28918d626e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a21b72097c9434f8953bbb993468098","max":642,"min":0,"orientation":"horizontal","style":"IPY_MODEL_64f5a1b4c60f40f2ae934d87468493a6","value":642}},"f6b659b42ffb456d93a09e70f5880a2e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
